{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"BOLD \u22ee <p> Knowledge Graph Exploration and Analysis platform </p> <p> Features \u2022   Quick Installation \u2022   Documentation \u2022   Demo </p>"},{"location":"#features","title":"Features","text":"<ul> <li>Seamless import of Knowledge Bases from LOD Cloud and Triply DB</li> <li>Interact with external SPARQL endpoints</li> <li>Create persistent reports and share them with others</li> <li>Run SPARQL or pre-built analysis queries</li> <li>Explore knowledge graph with interactive visualizations</li> <li>Pick unseen terms with fuzzy search</li> </ul>"},{"location":"#demo","title":"Demo","text":"<p>A live demo of BOLD can be found here.</p> <p>Log in with the following credentials: * Username: <code>demo</code> * Password: <code>demodemo</code></p>"},{"location":"#documentation","title":"Documentation","text":"<p>Visit BOLD documentation for more information.</p>"},{"location":"#quick-installation","title":"Quick Installation","text":"<p>You can quickly spin up a BOLD instance using Docker. Copy <code>docker-compose.full.yml</code> to <code>docker-compose.yml</code>. It provides all services that BOLD needs and runs BOLD itself. Use it by running <code>docker compose up -d</code>, you should see several services starting.</p> <p>Once they have all started, you should be able to access BOLD at <code>http://localhost:8000</code>.</p> <p>Log in with the following credentials: * Username: <code>admin</code> * Password: <code>admin</code></p>"},{"location":"#development-installation","title":"Development Installation","text":"<p>Copy <code>docker-compose.services.yml</code> to <code>docker-compose.yml</code>. Use it by running <code>docker compose up -d</code>, you should see several services starting. They will run in the background, the application will only work if they are running which you can check with <code>docker ps</code>. After following the preparation steps below once, you should be able to run <code>make start_dev</code> to start developing the project.</p>"},{"location":"#prepare-the-front-end","title":"Prepare the Front-end","text":"<p>Go into the <code>./frontend</code> folder. Make sure yarn is installed so you can do a <code>yarn install</code>.</p> <p>Checkout the avaiable scripts in the <code>package.json</code>, you should be able to run <code>yarn start</code> It should start a development server and open the webbrowser with the frontend. Make sure the backend is also running, see the preparation for it below.</p>"},{"location":"#prepare-the-back-end","title":"Prepare the Back-end","text":"<p>Go into the <code>./backend</code> folder. Make sure you have poetry for python installed, as you can install the project dependencies with it. Run <code>poetry install</code> to make it install the packages listed in <code>pyproject.toml</code>. It creates a virtual environment in which the dependencies are installed.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This work was supported by SIDN Fonds.</p> <p>This project is tested with BrowserStack</p>"},{"location":"CONTRIBUTING/","title":"Contributing Guidelines","text":"<p>If you are a first time contributor, start by reading this fantastic guide.</p>"},{"location":"CONTRIBUTING/#read-the-docs","title":"Read the docs","text":""},{"location":"CONTRIBUTING/#get-familliar-with-the-stack","title":"Get familliar with the stack","text":""},{"location":"CONTRIBUTING/#backend","title":"Backend:","text":"<ul> <li>Django</li> <li>Celery</li> <li>Blazegraph</li> </ul>"},{"location":"CONTRIBUTING/#frontend","title":"Frontend:","text":"<ul> <li>React</li> <li>React Router</li> <li>MUI</li> </ul>"},{"location":"CONTRIBUTING/#development-setup","title":"Development setup","text":"<p>For development purposes, you need to have a working Python and Rust installation.</p> <ul> <li>Set up local databases (See database setup)</li> <li>Install Rust and Python</li> <li>Build BOLD cli tools: <code>make release-tools</code></li> <li>Install Poetry</li> <li>Install necessary dependencies: <code>poetry install</code></li> <li>Run worker application: <code>make start_worker</code></li> <li>Run backend server: <code>make start_backend</code></li> <li>Run docs server: <code>make start_docs</code></li> <li>Open the BOLD web interface: http://localhost:8000/</li> </ul> <p>For further information proceed to Architechture guide.</p>"},{"location":"architecture/","title":"BOLD Architecture","text":"<p>This document is currently aimed at being a quick introduction to help a potentially new contributor understand how to navigate this project.</p> <p>The BOLD architecture is designed to be scaleable and to handle datasets of all sizes. Therefore we keep the data and the code separate.  Additionally, we define asynchronous execution architecture to allow for parallel execution of queries, dataset imports and fault tolerance.</p> <p>A high level overview of the architecture can be seen in the following figure.</p> <p></p> <p>In the above image we see the described separation. Keeping the analysis and visualization extensible is another important goal of the BOLD platform. To achieve this we aim to separete the analysis and visualization from the execution logic by strictly assigning these roles to the frontend and backend, respectively.</p> <p>The backend is responsible for the execution of queries and the data management. It recieves an execution plan in form a report and report cells containing sparql queries and executes them in parallel given the dataset. Similarly, the import logic does not work with third party services, but with rdf or sparql endpoint urls directly.</p> <p>The data is either stored in a self-hosted local data cluster or is accessed in a read-only fashion from a external data clusters using SPARQL and elasticsearch endpoints.</p> <p>State of the workers is decoupled from the backend server and vice-versa for fault tolerance.</p>"},{"location":"architecture/#import-flow","title":"Import Flow","text":"<p>While the import flow varies depending on the dataset and configuration, the general flow can be seen in the figure below.</p> <p></p> <p>The dataset urls are retrieved from a third party service (Linked Open Data Cloud or TriplyDB) and sent to the backend for import.</p> <p>The backend stores the import metadata and schedules an asynchronous import job which is passed to one of the worker nodes (see celery).</p> <p>The worker node is responsible for importing the dataset (if applicable) and importing it into the graph database (StarDog). Once the import is complete a search index is created by collecting all the entities from the dataset. Additionally, various queries are run to compute the dataset statistics and check the health of the dataset.</p> <p>Finally, the job results are persisted and the webserver is notified.</p>"},{"location":"architecture/#report-flow","title":"Report Flow","text":"<p>The report flow is inspired by the jupyter notebooks workflow. We have chosen to use our own workflow to allow easy addition of custom widgets, fuzzy term searching, self-hosting, and custom dataset import.</p> <p>Flow for a simple report cell execution is shown in the figure below.</p> <p></p> <p>Report executions consists of two parts.  (1) We save the report to run the up-to-date cells. (2) We execute the cells in parallel.</p> <p>Each cell execution job is passed onto a worker node (see celery). The tasks query graph database and search the database. Once the result is known, it is saved to the database and the webserver (as well as client) is notified.</p>"},{"location":"deployment/","title":"Deployment","text":"<p>It is advised to deploy BOLD using preconfigured docker containers.</p> <p>Create a <code>docker-compose.yml</code> file with the following contents: <pre><code>version: '3'\nservices:\n  bold:\n    image: egordm/ankc:latest\n    environment:\n      BLAZEGRAPH_ENABLE: on\n      BLAZEGRAPH_ENDPOINT: http://blazegraph:9999\n      MEILISEARCH_ENDPOINT: http://meilisearch:7700\n      MEILISEARCH_MASTER_KEY: masterKey\n    ports:\n      - 8001:8000\n    volumes:\n      - ./storage:/storage\n      - ./backend/.env:/app/.env\n    networks:\n      - bold-net\n    links:\n      - postgres\n      - blazegraph\n      - meilisearch\n    depends_on:\n      - postgres\n      - blazegraph\n      - meilisearch\n\n  postgres:\n    build: ./dev/postgres\n    environment:\n      POSTGRES_USER: root\n      POSTGRES_PASSWORD: helloworld\n      POSTGRES_MULTIPLE_DATABASES: test,develop,production\n    ports:\n      - 5432:5432\n    volumes:\n      - data-postgres:/var/lib/postgresql/data\n    networks:\n      - bold-net\n\n  blazegraph:\n    image: openkbs/blazegraph-docker\n    ports:\n      - 9999:9999\n    volumes:\n      - data-blazegraph:/var/lib/blazegraph/data\n      - ./storage:/storage\n      - ./dev/blazegraph:/opt/blazegraph-custom\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"-f\", \"http://localhost:9999\" ]\n      interval: 3s\n      timeout: 5s\n      retries: 3\n    networks:\n      - bold-net\n\n  meilisearch:\n    image: getmeili/meilisearch:v1.7.2\n    ports:\n      - 7700:7700\n    environment:\n      MEILI_NO_ANALYTICS: true\n      MEILI_MASTER_KEY: \"masterKey\"\n    volumes:\n      - data-meilisearch:/meili_data\n    networks:\n      - bold-net\n\n\nvolumes:\n  data-postgres:\n  data-blazegraph:\n  data-meilisearch:\n\nnetworks:\n  bold-net:\n</code></pre></p> <p>Create a <code>backend/.env</code> file with the following contents: <pre><code>OPENAPI_KEY=\"(optional) key for the openai api\"\nDEBUG=off\nSTARDOG_ENABLE=on\n</code></pre></p> <p>Change <code>BLAZEGRAPH_ENABLE</code> to <code>off</code> if you don't want to use Blazegraph.</p> <p>Then run <code>docker-compose up -d</code> to start the container. You can now access BOLD at <code>http://localhost:8000</code>.</p>"},{"location":"deployment/#system-requirements","title":"System Requirements","text":"<p>The server requirements are mostly bound by the Blazegraph database.</p> <p>You can choose to not use the Blazegraph database, but you will not be able to import the full datasets (only external SPARQL endpoints are allowed). Moreover you can decide to run Blazegraph on a different machine.</p> <ul> <li>You must have twice the amount of storage your datasets require. (YAGO is 60Gb thus 120Gb)</li> <li>You must allocate at least 2 cores for the server.</li> <li>Memory requirements are found below:</li> </ul> Number of Triples Total System Memory 100 million 8G 1 billion 32G 10 billion 128G 25 billion 256G 50 billion 512G"},{"location":"installation/","title":"Installation","text":"<p>This document is describes how to install BOLD for typical usage. If you want to install BOLD for development purposes, we refer you to the CONTRIBUTING section.</p> <p>The BOLD platform depends on postgresql and blazegraph databases for knowledge graph and state storage. In the following steps we discuss their setup as well as necessary steps to get BOLD up and running.</p>"},{"location":"installation/#database-setup","title":"Database Setup","text":"<p>If you have already postgresql and blazegraph databases installed, you can skip this step. In this step we discuss their setup as docker containers.</p> <ul> <li>Install Docker and Docker Compose</li> <li>Build the Docker images: <code>docker compose build</code></li> <li>Start the database services: <code>docker compose up</code></li> </ul>"},{"location":"installation/#pull-docker-image-from-docker-hub-for-demonstration","title":"Pull docker image from docker hub (for demonstration)","text":"<p>If you want to install BOLD for development purposes, we refer you to the CONTRIBUTING section. This step will pull the latest deployed version from docker hub, which should be a working version of BOLD.</p> <ul> <li>Build docker images: <code>docker compose -f docker-compose.full.yml build</code></li> <li>Start BOLD and the relevant services: <code>docker compose -f docker-compose.full.yml up</code></li> <li>Open the BOLD web interface: http://127.0.0.1:8000/</li> </ul>"},{"location":"installation/#build-docker-image-from-source-for-development","title":"Build docker image from source (for development)","text":"<p>Build docker image from source if you want to run newest or modified version of BOLD. In this step we describe steps on how to run BOLD as a docker container.</p> <ul> <li>Build docker images: <code>docker compose -f docker-compose.services.yml -f docker-compose.standalone.yml build</code></li> <li>Start BOLD and the relevant services: <code>docker compose -f docker-compose.standalone.yml -f docker-compose.services.yml up</code></li> <li>Open the BOLD web interface: http://127.0.0.1:8000/</li> </ul>"},{"location":"installation/#enabling-gpt-code-completion","title":"Enabling GPT code completion","text":"<p>To enable code completion with GPT you need an OpenAI API key. If you don't have one, you can request one at openai.com. Once you have a key, create file <code>backend/.env</code> and add the following line:</p> <pre><code>OPENAPI_KEY=&lt;your-key&gt;\n</code></pre>"},{"location":"installation/#configuration","title":"Configuration","text":"<p>You can configure your bold installation by creating <code>backend/.env</code> file and setting following variables:</p> <ul> <li><code>DEBUG</code>: Set to <code>False</code> to disable debug mode.</li> <li><code>BLAZEGRAPH_ENABLE</code>: Set to <code>False</code> to disable local dataset downloads and search indexing. This is useful if you don't have a blazegraph instance running.</li> <li><code>MEILISEARCH_ENDPOINT</code>: Set to the endpoint of your meilisearch instance. (default: <code>http://meilisearch:7700</code>)</li> <li><code>BLAZEGRAPH_ENDPOINT</code>: Set to the endpoint of your blazegraph instance. (default: <code>http://blazegraph:9999</code>)</li> <li><code>DJANGO_SUPERUSER_USERNAME</code>: Set to the username of the superuser. (default: <code>admin</code>)</li> <li><code>DJANGO_SUPERUSER_PASSWORD</code>: Set to the password of the superuser. (default: <code>admin</code>)</li> <li><code>DJANGO_SUPERUSER_EMAIL</code>: Set to the email of the superuser.</li> </ul>"},{"location":"structure/","title":"Project Structure","text":"<p>This document is aimed to be a quick introduction to help a potentially new contributor understand how to navigate this project.</p>"},{"location":"structure/#file-organization","title":"File Organization","text":"<p>The projects files are organized in the following way:</p> <ul> <li><code>backend</code>: Contains the <code>Django</code> backend and worker source code.</li> <li><code>dev</code>: Contains the development scripts and docker configs.</li> <li><code>docs</code>: Contains the <code>mkdocs</code> documentation.</li> <li><code>frontend</code>: Contains the <code>react</code> frontend source code.</li> <li><code>storage</code>: Is the directory where the BOLD runtime files are stored.</li> <li><code>tools</code>: Contains <code>rust</code> based BOLD cli tools for RDF preprocessing and search indexing.</li> </ul>"},{"location":"structure/#backend-structure","title":"Backend Structure","text":"<p>The BOLD backend is split into multiple modules that are responsible for different parts of the BOLD system.</p> <ul> <li><code>backend</code>: Contains the <code>Django</code> and <code>celery</code> configuration.</li> <li><code>datasets</code>: Is responsible for all dataset related operations such as import and management.</li> <li><code>frontend</code>: Contains static files generated by the <code>react</code> frontend.</li> <li><code>reports</code>: Is responsible for all report related operations such as management and cell execution.</li> <li><code>shared</code>: Contains shared utility code used by all the modules.</li> <li><code>tasks</code>: Contains framework for asynchronous execution and tracking of tasks.</li> <li><code>users</code>: Contains all user related operations such as authentication and management.</li> </ul>"},{"location":"structure/#frontend-structure","title":"Frontend Structure","text":"<p>The BOLD frontend follows standard react and typescript conventions for code organization.</p> <ul> <li><code>components</code>: Contains all the UI components which can be used in any context.</li> <li><code>containers</code>: Contains all the UI containers which combine multiple components into combined user interfaces.</li> <li><code>hooks</code>: Contains hooks for data from state extraction.</li> <li><code>pages</code>: Holds complete pages used in navigation and frontend.</li> <li><code>providers</code>: Contains all the data providers and hooks used for managing state of the application.</li> <li><code>services</code>: Holds service apis for the backend, LODC and TriplyDB.</li> <li><code>theme</code>: Contains the mui theme configuration.</li> <li><code>types</code>: Contains typescript type definitions for data structures used in the application.</li> <li><code>utils</code>: Contains utility functions.</li> <li><code>App.tsx</code>: The main entry point for the application.</li> </ul>"},{"location":"structure/#models","title":"Models","text":"<ul> <li>Dataset - The dataset model is used to manage the data sources and local datasets. It stores handles to local and remote resources.</li> <li>Report - The report model is used to manage the reports. It stores the cells and the results of the cells in the notebook json field.</li> <li>Task - The task model represents a scheduled task. It stores a handle to the celery task and the information about the task status.</li> </ul>"},{"location":"user_manual/","title":"User Manual","text":"<p>This document is aimed at new users who want learn or get an overview of the BOLD platform features.</p>"},{"location":"user_manual/#definitions","title":"Definitions","text":""},{"location":"user_manual/#dataset","title":"Dataset","text":"<p>A BOLD dataset represents an (imported) RDF dataset. </p> <p>The dataset consists of a SPARQL endpoint and a search index. Both can point to either a local of a remote resource.</p> <p>The datasets can be shared between users and reports.</p>"},{"location":"user_manual/#report","title":"Report","text":"<p>A BOLD report represents a collection of cells that contain SPARQL queries and widgets. A report also persists all the query and widget results so that they can be reviewed later.</p>"},{"location":"user_manual/#report-cell","title":"Report Cell","text":"<p>A report cell contains either SPARQL queries or a configurable widget which generates queries for the database.</p>"},{"location":"user_manual/#task","title":"Task","text":"<p>A BOLD task represents a collection of work that can be scheduled and assigned to a worker. Tasks are meant to be used for long-running tasks and run in parallel to avoid blocking the main server.</p>"},{"location":"user_manual/#basic-navigation","title":"Basic Navigation","text":"<p>Navigate through the app by using the sidebar.</p> <ul> <li>Reports - Page to manage the reports.</li> <li>Datasets - Page to manage the datasets.</li> <li>Tasks - Page to view scheduled/completed tasks.</li> <li>LODC - Browse the LODC datasets</li> <li>TriplyDB - Browse the TriplyDB datasets</li> </ul> <p></p> <p>Filter the entities by defining custom filters and select specific columns to display.</p> <p></p>"},{"location":"user_manual/#task-bar","title":"Task bar","text":"<p>The task bar is located in the bottom right corner of the screen and shows the progress of currently running tasks. You can expand it to view the details of the running or completed tasks.</p> <p></p>"},{"location":"user_manual/#importing-datasets","title":"Importing Datasets","text":"<p>The datasets can be imported a multitude of sources. There are two main ways to import datasets in BOLD.</p> <ul> <li>RDF Dataset - Import an RDF dataset by importing it into a local dataset and creating a search index. This requires the machine to have sufficient memory and storage to store the dataset.</li> <li>SPARQL Endpoint - Create a dataset from SPARQL endpoint and using an external search index. This does not require machine to have enough storage to store the dataset as it is accessed directly from the endpoint.</li> </ul>"},{"location":"user_manual/#importing-from-linked-open-data-cloud","title":"Importing from Linked Open Data Cloud","text":"<p>Import a dataset from (linked open data cloud)[https://lod-cloud.net/datasets]. Since LODC stores links to external resources check whether the file is accessible before importing it by clicking the download link.</p> <p></p>"},{"location":"user_manual/#importing-from-triply-db","title":"Importing from Triply DB","text":"<p>Import a dataset from (Triply DB)[https://triplydb.com/]. When importing a SPARQL endpoint dataset from TriplyDB check whether a sparql service and elastic search services are running by visiting the dataset page (\"services\" tab).</p> <p></p>"},{"location":"user_manual/#importing-from-sparql-endpoints","title":"Importing from SPARQL endpoints","text":"<p>Importing from manual sparql endpoints currently only supports Wikimedia search api. If your dataset does not contain wikimedia data, then you are limited to simple code cell execution.</p> <p></p>"},{"location":"user_manual/#dataset-actions","title":"Dataset Actions","text":"<p>Dataset deletion deletes the local dataset and the search index.</p> <p></p>"},{"location":"user_manual/#creating-a-report","title":"Creating a Report","text":"<p>Create a report by selecting a dataset and adding cells to it.</p> <p></p>"},{"location":"user_manual/#report-cell-types","title":"Report Cell Types","text":"<p>Report consists of cells that query the dataset SPARQL and search index and display the results in a persistent way.</p>"},{"location":"user_manual/#prefixes","title":"Prefixes","text":"<p>RDF iris are verbose and can be confusing to users. Define custom prefixes to shorten and make the iris more readable.</p> <p></p>"},{"location":"user_manual/#code-cell","title":"Code Cell","text":"<p>Code cells allow you to execute SPARQL queries. The editor is powered by yasqui. The defined prefixes can be used in autocompletion.</p> <p>See yasgui documentation to find out how to visualize your results in a more coherent way.</p> <p></p>"},{"location":"user_manual/#code-generation-using-gpt","title":"Code Generation using GPT","text":"<p>Writing SPARQL queries is a tedious task and is not always straightforward as it may require some knowledge of the underlying dataset.</p> <p>To make it easier to write SPARQL queries, BOLD provides a utility to promt GPT to generate SPARQL queries for you. The specified prompt as well as the defined prefixes are used as context for the generation.</p> <p></p>"},{"location":"user_manual/#markdown-cell","title":"Markdown Cell","text":"<p>You can write down your thoughts and insights in markdown cells. Use the buttons in the toolbar on the right to show/hide the markdown source code.</p> <p></p>"},{"location":"user_manual/#class-browser-widget","title":"Class Browser Widget","text":"<p>Class browser counts the number of instances for each class and displays them in a hierarchical structure. Each type in the plot is sized proportionally to the number of instances it contains.</p> <p>For larger datasets, it may be useful to increase the limit of the query to get a better overview of the class hierarchy.</p> <p></p>"},{"location":"user_manual/#histogram-widget","title":"Histogram Widget","text":"<p>The histogram widget displays the distribution of values for a certain property given a set of filters.  This cell is perfect for finding biases in the data or getting important insights.</p> <p>Various options are available to customize the histogram:</p> <ul> <li>Group values of - property whose values are used in the histogram</li> <li>Filters - filters to reduce number of matching entities before the histogram is generated</li> <li>Continuous - whether property values are continuous (e.g. interger, decimal, date) or discrete</li> <li>Limit number of groups - limits the number of bins in the histogram</li> <li>Min group size - minimum number of entities required to generate a group in the histogram</li> <li>Temporal grouping - Adds a third dimension to the histogram by creating a histogram for each time period given a temporal property.</li> </ul> <p></p> <p>The results are displayed in four tabs:</p> <ul> <li>Plot - The generated histogram</li> <li>Table - The data used to generate the histogram</li> <li>Examples - List of examples of entities that match the filters</li> <li>Completeness Analysis - Displays a ratio of entities that match the filters and have a defined property value versus the entities that don't have the property value.</li> </ul> <p>Note: Completeness analysis counts amount of entities, meaning that multiple triples corresponding to the same subject won't be counted twice.</p>"},{"location":"user_manual/#query-builder","title":"Query Builder","text":"<p>Query Builder provides a simple interface to create SPARQL queries. The default search input allows you to fuzzy search for relevant entities and properties. Create filters to filter resulting entities and select the properties to be used in the plot.</p> <p>The results are displayed in a table below.</p> <p></p>"},{"location":"user_manual/#plot-builder","title":"Plot Builder","text":"<p>The plot builder widget allows you to create custom plots using the data from the dataset.</p> <p>Create filters to filter resulting entities and select the properties to be used in the plot. If you want to build a 3d plot (x, y and groups) select the \"Is XYZ\" option. </p> <p>This widget is compatible with Query Builder widget and can be converted from/to Query Builder widget.</p> <p></p>"},{"location":"user_manual/#entity-properties","title":"Entity Properties","text":"<p>The properties cell displays the properties of a given entity.</p> <p></p>"},{"location":"user_manual/#subgraph-view","title":"Subgraph View","text":"<p>The subgraph cell displays arbitrary depth the subgraph of a given entity by using properties as egdes.</p> <p>Various options are available to customize the subgraph:</p> <ul> <li>Target Entity - The entity to start the subgraph from</li> <li>Use any property - Include all applicable properties in the subgraph or specify specific properties to use</li> <li>Limit results - Limits the number of nodes displayed</li> <li>Limit depth - Limits the depth of the subgraph</li> </ul> <p></p>"},{"location":"user_manual/#class-statistics","title":"Class Statistics","text":"<p>The class tree cell displays the class hierarchy of the classes and properties in the dataset. It can be used to get important insights in new data user wants to explore.</p> <p>Various options are available to customize the class tree:</p> <ul> <li>With counts - Count the number of entities for each class and property'</li> <li>With subclasses - Include class subclasses in the tree</li> <li>With equivalent classes - Include equivalent classes in the tree</li> <li>With keys - Include keys in the tree for other reources</li> <li>With properties - Include class properties in the tree</li> <li>Use owl classes - Include owl classes in the tree</li> <li>Use rdf classes - Include rdf classes in the tree</li> <li>Use any classes - Include all classes in the tree</li> </ul> <p></p>"},{"location":"user_manual/#downloading-and-importing-notebooks","title":"Downloading and Importing Notebooks","text":"<p>Notebooks can be downloaded as JSON files within the sharing dialog by clicking on the download button.</p> <p></p> <p>You can import a notebook's content by opening an existing notebook and clicking on the import button.</p> <p></p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>datasets<ul> <li>management<ul> <li>commands<ul> <li>import_kg</li> </ul> </li> </ul> </li> <li>models</li> <li>serializers</li> <li>services<ul> <li>blazegraph</li> <li>meilisearch</li> <li>query</li> <li>search</li> </ul> </li> <li>tasks<ul> <li>imports</li> <li>maintenance</li> <li>pipeline</li> <li>processing</li> </ul> </li> <li>views<ul> <li>datasets</li> <li>lodc</li> </ul> </li> </ul> </li> <li>reports<ul> <li>consumers</li> <li>models</li> <li>permissions</li> <li>serializers</li> <li>tasks</li> <li>views</li> </ul> </li> <li>shared<ul> <li>dict</li> <li>logging</li> <li>models</li> <li>paths</li> <li>query</li> <li>random</li> <li>shell</li> <li>websocket</li> </ul> </li> <li>tasks<ul> <li>apps</li> <li>consumers</li> <li>management<ul> <li>commands<ul> <li>celery_worker</li> </ul> </li> </ul> </li> <li>models</li> <li>serializers</li> <li>signals</li> <li>utils</li> <li>views</li> </ul> </li> <li>users<ul> <li>middleware</li> <li>mixins</li> <li>permissions</li> <li>serializers</li> <li>views</li> </ul> </li> </ul>"},{"location":"reference/datasets/","title":"datasets","text":""},{"location":"reference/datasets/models/","title":"models","text":""},{"location":"reference/datasets/models/#datasets.models.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>TaskMixin</code>, <code>TimeStampMixin</code>, <code>OwnableMixin</code></p> <p>The internal dataset model.</p> Source code in <code>backend/datasets/models.py</code> <pre><code>class Dataset(TaskMixin, TimeStampMixin, OwnableMixin):\n    \"\"\"\n    The internal dataset model.\n    \"\"\"\n    STATES = ((state.value, state.value) for state in DatasetState)\n\n    class Mode(models.TextChoices):\n        \"\"\"\n        The Mode class is an enumeration of the possible modes of a dataset\n        \"\"\"\n        LOCAL = 'LOCAL', _('Imported locally ')\n        SPARQL = 'SPARQL', _('From SPARQL endpoint')\n\n    class SearchMode(models.TextChoices):\n        \"\"\"\n        The SearchMode class is an enumeration of the possible search modes of a dataset\n        \"\"\"\n        LOCAL = 'LOCAL', _('Imported locally ')\n        WIKIDATA = 'WIKIDATA', _('From Wikidata')\n        TRIPLYDB = 'TRIPLYDB', _('From TripyDB')\n\n    id = models.UUIDField(default=uuid.uuid4, primary_key=True)\n    \"\"\"The identifier of the dataset.\"\"\"\n    name = models.CharField(max_length=255)\n    \"\"\"The name of the dataset.\"\"\"\n    description = models.TextField(blank=True)\n    \"\"\"The description of the dataset.\"\"\"\n    source = models.JSONField()\n    \"\"\"The source of the dataset.\"\"\"\n    mode = models.CharField(max_length=255, choices=Mode.choices, default=Mode.LOCAL)\n    \"\"\"The mode of the dataset.\"\"\"\n    search_mode = models.CharField(max_length=255, choices=SearchMode.choices, default=SearchMode.LOCAL)\n    \"\"\"The search mode of the dataset.\"\"\"\n    creator = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.SET_NULL, null=True)\n    \"\"\"The user who created the dataset.\"\"\"\n\n    local_database: str = models.CharField(max_length=255, null=True)\n    \"\"\"The local blazegraph database identifier of the dataset.\"\"\"\n    sparql_endpoint = models.CharField(max_length=255, null=True)\n    \"\"\"The SPARQL endpoint of the dataset.\"\"\"\n\n    statistics = models.JSONField(null=True)\n    \"\"\"The statistics of the dataset.\"\"\"\n    namespaces = models.JSONField(null=True)\n    \"\"\"The list of sparql namespaces/prefixes in the dataset.\"\"\"\n    state = models.CharField(choices=STATES, default=DatasetState.QUEUED.value, max_length=255)\n    \"\"\"The import state of the dataset.\"\"\"\n    import_task = models.OneToOneField('tasks.Task', on_delete=models.SET_NULL, null=True)\n    \"\"\"The import task of the dataset.\"\"\"\n\n    objects = models.Manager()\n\n    @property\n    def search_index_name(self) -&gt; str:\n        \"\"\"\n        The path to the search index of the dataset.\n        :return:\n        \"\"\"\n        return self.local_database if self.local_database else None\n\n    def get_search_service(self) -&gt; SearchService:\n        \"\"\"\n        Return appropriate search service depending on the search mode\n        \"\"\"\n        match self.search_mode:\n            case self.SearchMode.LOCAL:\n                if not self.search_index_name:\n                    raise Exception('Dataset search index has not been created yet')\n                return LocalSearchService(index_name=self.search_index_name)\n            case self.SearchMode.WIKIDATA:\n                return WikidataSearchService()\n            case self.SearchMode.TRIPLYDB:\n                if 'tdb_id' not in self.source:\n                    raise Exception('Dataset is not a TriplyDB dataset')\n                return TriplyDBSearchService(self.source['tdb_id'])\n            case _:\n                raise ValueError(f'Unknown search mode {self.search_mode}')\n\n    def get_query_service(self) -&gt; QueryService:\n        \"\"\"\n        If the mode is local, return a local query service, otherwise return a SPARQL query service\n        \"\"\"\n        match self.mode:\n            case self.Mode.LOCAL:\n                if not self.local_database:\n                    raise Exception('Dataset local database has not been imported yet')\n                return LocalQueryService(str(self.local_database))\n            case self.Mode.SPARQL:\n                return SPARQLQueryService(str(self.sparql_endpoint))\n            case _:\n                raise ValueError(f'Unknown mode {self.mode}')\n\n    def can_view(self, user: User):\n        return bool(user)\n\n    def can_edit(self, user: User):\n        return super().can_edit(user) or self.creator == user\n</code></pre>"},{"location":"reference/datasets/models/#datasets.models.Dataset.creator","title":"<code>creator = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.SET_NULL, null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The user who created the dataset.</p>"},{"location":"reference/datasets/models/#datasets.models.Dataset.description","title":"<code>description = models.TextField(blank=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The description of the dataset.</p>"},{"location":"reference/datasets/models/#datasets.models.Dataset.id","title":"<code>id = models.UUIDField(default=uuid.uuid4, primary_key=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The identifier of the dataset.</p>"},{"location":"reference/datasets/models/#datasets.models.Dataset.import_task","title":"<code>import_task = models.OneToOneField('tasks.Task', on_delete=models.SET_NULL, null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The import task of the dataset.</p>"},{"location":"reference/datasets/models/#datasets.models.Dataset.local_database","title":"<code>local_database: str = models.CharField(max_length=255, null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The local blazegraph database identifier of the dataset.</p>"},{"location":"reference/datasets/models/#datasets.models.Dataset.mode","title":"<code>mode = models.CharField(max_length=255, choices=Mode.choices, default=Mode.LOCAL)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The mode of the dataset.</p>"},{"location":"reference/datasets/models/#datasets.models.Dataset.name","title":"<code>name = models.CharField(max_length=255)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the dataset.</p>"},{"location":"reference/datasets/models/#datasets.models.Dataset.namespaces","title":"<code>namespaces = models.JSONField(null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The list of sparql namespaces/prefixes in the dataset.</p>"},{"location":"reference/datasets/models/#datasets.models.Dataset.search_index_name","title":"<code>search_index_name: str</code>  <code>property</code>","text":"<p>The path to the search index of the dataset. :return:</p>"},{"location":"reference/datasets/models/#datasets.models.Dataset.search_mode","title":"<code>search_mode = models.CharField(max_length=255, choices=SearchMode.choices, default=SearchMode.LOCAL)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The search mode of the dataset.</p>"},{"location":"reference/datasets/models/#datasets.models.Dataset.source","title":"<code>source = models.JSONField()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The source of the dataset.</p>"},{"location":"reference/datasets/models/#datasets.models.Dataset.sparql_endpoint","title":"<code>sparql_endpoint = models.CharField(max_length=255, null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The SPARQL endpoint of the dataset.</p>"},{"location":"reference/datasets/models/#datasets.models.Dataset.state","title":"<code>state = models.CharField(choices=STATES, default=DatasetState.QUEUED.value, max_length=255)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The import state of the dataset.</p>"},{"location":"reference/datasets/models/#datasets.models.Dataset.statistics","title":"<code>statistics = models.JSONField(null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The statistics of the dataset.</p>"},{"location":"reference/datasets/models/#datasets.models.Dataset.Mode","title":"<code>Mode</code>","text":"<p>               Bases: <code>TextChoices</code></p> <p>The Mode class is an enumeration of the possible modes of a dataset</p> Source code in <code>backend/datasets/models.py</code> <pre><code>class Mode(models.TextChoices):\n    \"\"\"\n    The Mode class is an enumeration of the possible modes of a dataset\n    \"\"\"\n    LOCAL = 'LOCAL', _('Imported locally ')\n    SPARQL = 'SPARQL', _('From SPARQL endpoint')\n</code></pre>"},{"location":"reference/datasets/models/#datasets.models.Dataset.SearchMode","title":"<code>SearchMode</code>","text":"<p>               Bases: <code>TextChoices</code></p> <p>The SearchMode class is an enumeration of the possible search modes of a dataset</p> Source code in <code>backend/datasets/models.py</code> <pre><code>class SearchMode(models.TextChoices):\n    \"\"\"\n    The SearchMode class is an enumeration of the possible search modes of a dataset\n    \"\"\"\n    LOCAL = 'LOCAL', _('Imported locally ')\n    WIKIDATA = 'WIKIDATA', _('From Wikidata')\n    TRIPLYDB = 'TRIPLYDB', _('From TripyDB')\n</code></pre>"},{"location":"reference/datasets/models/#datasets.models.Dataset.get_query_service","title":"<code>get_query_service()</code>","text":"<p>If the mode is local, return a local query service, otherwise return a SPARQL query service</p> Source code in <code>backend/datasets/models.py</code> <pre><code>def get_query_service(self) -&gt; QueryService:\n    \"\"\"\n    If the mode is local, return a local query service, otherwise return a SPARQL query service\n    \"\"\"\n    match self.mode:\n        case self.Mode.LOCAL:\n            if not self.local_database:\n                raise Exception('Dataset local database has not been imported yet')\n            return LocalQueryService(str(self.local_database))\n        case self.Mode.SPARQL:\n            return SPARQLQueryService(str(self.sparql_endpoint))\n        case _:\n            raise ValueError(f'Unknown mode {self.mode}')\n</code></pre>"},{"location":"reference/datasets/models/#datasets.models.Dataset.get_search_service","title":"<code>get_search_service()</code>","text":"<p>Return appropriate search service depending on the search mode</p> Source code in <code>backend/datasets/models.py</code> <pre><code>def get_search_service(self) -&gt; SearchService:\n    \"\"\"\n    Return appropriate search service depending on the search mode\n    \"\"\"\n    match self.search_mode:\n        case self.SearchMode.LOCAL:\n            if not self.search_index_name:\n                raise Exception('Dataset search index has not been created yet')\n            return LocalSearchService(index_name=self.search_index_name)\n        case self.SearchMode.WIKIDATA:\n            return WikidataSearchService()\n        case self.SearchMode.TRIPLYDB:\n            if 'tdb_id' not in self.source:\n                raise Exception('Dataset is not a TriplyDB dataset')\n            return TriplyDBSearchService(self.source['tdb_id'])\n        case _:\n            raise ValueError(f'Unknown search mode {self.search_mode}')\n</code></pre>"},{"location":"reference/datasets/models/#datasets.models.DatasetState","title":"<code>DatasetState</code>","text":"<p>               Bases: <code>Enum</code></p> <p>The DatasetState class is an enumeration of the possible states of a dataset</p> Source code in <code>backend/datasets/models.py</code> <pre><code>class DatasetState(Enum):\n    \"\"\"\n    The DatasetState class is an enumeration of the possible states of a dataset\n    \"\"\"\n    QUEUED = 'QUEUED'\n    IMPORTING = 'IMPORTING'\n    IMPORTED = 'IMPORTED'\n    FAILED = 'FAILED'\n</code></pre>"},{"location":"reference/datasets/serializers/","title":"serializers","text":""},{"location":"reference/datasets/management/","title":"management","text":""},{"location":"reference/datasets/management/commands/","title":"commands","text":""},{"location":"reference/datasets/management/commands/import_kg/","title":"import_kg","text":""},{"location":"reference/datasets/services/","title":"services","text":""},{"location":"reference/datasets/services/blazegraph/","title":"blazegraph","text":""},{"location":"reference/datasets/services/meilisearch/","title":"meilisearch","text":""},{"location":"reference/datasets/services/query/","title":"query","text":""},{"location":"reference/datasets/services/search/","title":"search","text":""},{"location":"reference/datasets/tasks/","title":"tasks","text":""},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>TaskMixin</code>, <code>TimeStampMixin</code>, <code>OwnableMixin</code></p> <p>The internal dataset model.</p> Source code in <code>backend/datasets/models.py</code> <pre><code>class Dataset(TaskMixin, TimeStampMixin, OwnableMixin):\n    \"\"\"\n    The internal dataset model.\n    \"\"\"\n    STATES = ((state.value, state.value) for state in DatasetState)\n\n    class Mode(models.TextChoices):\n        \"\"\"\n        The Mode class is an enumeration of the possible modes of a dataset\n        \"\"\"\n        LOCAL = 'LOCAL', _('Imported locally ')\n        SPARQL = 'SPARQL', _('From SPARQL endpoint')\n\n    class SearchMode(models.TextChoices):\n        \"\"\"\n        The SearchMode class is an enumeration of the possible search modes of a dataset\n        \"\"\"\n        LOCAL = 'LOCAL', _('Imported locally ')\n        WIKIDATA = 'WIKIDATA', _('From Wikidata')\n        TRIPLYDB = 'TRIPLYDB', _('From TripyDB')\n\n    id = models.UUIDField(default=uuid.uuid4, primary_key=True)\n    \"\"\"The identifier of the dataset.\"\"\"\n    name = models.CharField(max_length=255)\n    \"\"\"The name of the dataset.\"\"\"\n    description = models.TextField(blank=True)\n    \"\"\"The description of the dataset.\"\"\"\n    source = models.JSONField()\n    \"\"\"The source of the dataset.\"\"\"\n    mode = models.CharField(max_length=255, choices=Mode.choices, default=Mode.LOCAL)\n    \"\"\"The mode of the dataset.\"\"\"\n    search_mode = models.CharField(max_length=255, choices=SearchMode.choices, default=SearchMode.LOCAL)\n    \"\"\"The search mode of the dataset.\"\"\"\n    creator = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.SET_NULL, null=True)\n    \"\"\"The user who created the dataset.\"\"\"\n\n    local_database: str = models.CharField(max_length=255, null=True)\n    \"\"\"The local blazegraph database identifier of the dataset.\"\"\"\n    sparql_endpoint = models.CharField(max_length=255, null=True)\n    \"\"\"The SPARQL endpoint of the dataset.\"\"\"\n\n    statistics = models.JSONField(null=True)\n    \"\"\"The statistics of the dataset.\"\"\"\n    namespaces = models.JSONField(null=True)\n    \"\"\"The list of sparql namespaces/prefixes in the dataset.\"\"\"\n    state = models.CharField(choices=STATES, default=DatasetState.QUEUED.value, max_length=255)\n    \"\"\"The import state of the dataset.\"\"\"\n    import_task = models.OneToOneField('tasks.Task', on_delete=models.SET_NULL, null=True)\n    \"\"\"The import task of the dataset.\"\"\"\n\n    objects = models.Manager()\n\n    @property\n    def search_index_name(self) -&gt; str:\n        \"\"\"\n        The path to the search index of the dataset.\n        :return:\n        \"\"\"\n        return self.local_database if self.local_database else None\n\n    def get_search_service(self) -&gt; SearchService:\n        \"\"\"\n        Return appropriate search service depending on the search mode\n        \"\"\"\n        match self.search_mode:\n            case self.SearchMode.LOCAL:\n                if not self.search_index_name:\n                    raise Exception('Dataset search index has not been created yet')\n                return LocalSearchService(index_name=self.search_index_name)\n            case self.SearchMode.WIKIDATA:\n                return WikidataSearchService()\n            case self.SearchMode.TRIPLYDB:\n                if 'tdb_id' not in self.source:\n                    raise Exception('Dataset is not a TriplyDB dataset')\n                return TriplyDBSearchService(self.source['tdb_id'])\n            case _:\n                raise ValueError(f'Unknown search mode {self.search_mode}')\n\n    def get_query_service(self) -&gt; QueryService:\n        \"\"\"\n        If the mode is local, return a local query service, otherwise return a SPARQL query service\n        \"\"\"\n        match self.mode:\n            case self.Mode.LOCAL:\n                if not self.local_database:\n                    raise Exception('Dataset local database has not been imported yet')\n                return LocalQueryService(str(self.local_database))\n            case self.Mode.SPARQL:\n                return SPARQLQueryService(str(self.sparql_endpoint))\n            case _:\n                raise ValueError(f'Unknown mode {self.mode}')\n\n    def can_view(self, user: User):\n        return bool(user)\n\n    def can_edit(self, user: User):\n        return super().can_edit(user) or self.creator == user\n</code></pre>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.creator","title":"<code>creator = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.SET_NULL, null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The user who created the dataset.</p>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.description","title":"<code>description = models.TextField(blank=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The description of the dataset.</p>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.id","title":"<code>id = models.UUIDField(default=uuid.uuid4, primary_key=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The identifier of the dataset.</p>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.import_task","title":"<code>import_task = models.OneToOneField('tasks.Task', on_delete=models.SET_NULL, null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The import task of the dataset.</p>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.local_database","title":"<code>local_database: str = models.CharField(max_length=255, null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The local blazegraph database identifier of the dataset.</p>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.mode","title":"<code>mode = models.CharField(max_length=255, choices=Mode.choices, default=Mode.LOCAL)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The mode of the dataset.</p>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.name","title":"<code>name = models.CharField(max_length=255)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the dataset.</p>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.namespaces","title":"<code>namespaces = models.JSONField(null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The list of sparql namespaces/prefixes in the dataset.</p>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.search_index_name","title":"<code>search_index_name: str</code>  <code>property</code>","text":"<p>The path to the search index of the dataset. :return:</p>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.search_mode","title":"<code>search_mode = models.CharField(max_length=255, choices=SearchMode.choices, default=SearchMode.LOCAL)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The search mode of the dataset.</p>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.source","title":"<code>source = models.JSONField()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The source of the dataset.</p>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.sparql_endpoint","title":"<code>sparql_endpoint = models.CharField(max_length=255, null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The SPARQL endpoint of the dataset.</p>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.state","title":"<code>state = models.CharField(choices=STATES, default=DatasetState.QUEUED.value, max_length=255)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The import state of the dataset.</p>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.statistics","title":"<code>statistics = models.JSONField(null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The statistics of the dataset.</p>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.Mode","title":"<code>Mode</code>","text":"<p>               Bases: <code>TextChoices</code></p> <p>The Mode class is an enumeration of the possible modes of a dataset</p> Source code in <code>backend/datasets/models.py</code> <pre><code>class Mode(models.TextChoices):\n    \"\"\"\n    The Mode class is an enumeration of the possible modes of a dataset\n    \"\"\"\n    LOCAL = 'LOCAL', _('Imported locally ')\n    SPARQL = 'SPARQL', _('From SPARQL endpoint')\n</code></pre>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.SearchMode","title":"<code>SearchMode</code>","text":"<p>               Bases: <code>TextChoices</code></p> <p>The SearchMode class is an enumeration of the possible search modes of a dataset</p> Source code in <code>backend/datasets/models.py</code> <pre><code>class SearchMode(models.TextChoices):\n    \"\"\"\n    The SearchMode class is an enumeration of the possible search modes of a dataset\n    \"\"\"\n    LOCAL = 'LOCAL', _('Imported locally ')\n    WIKIDATA = 'WIKIDATA', _('From Wikidata')\n    TRIPLYDB = 'TRIPLYDB', _('From TripyDB')\n</code></pre>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.get_query_service","title":"<code>get_query_service()</code>","text":"<p>If the mode is local, return a local query service, otherwise return a SPARQL query service</p> Source code in <code>backend/datasets/models.py</code> <pre><code>def get_query_service(self) -&gt; QueryService:\n    \"\"\"\n    If the mode is local, return a local query service, otherwise return a SPARQL query service\n    \"\"\"\n    match self.mode:\n        case self.Mode.LOCAL:\n            if not self.local_database:\n                raise Exception('Dataset local database has not been imported yet')\n            return LocalQueryService(str(self.local_database))\n        case self.Mode.SPARQL:\n            return SPARQLQueryService(str(self.sparql_endpoint))\n        case _:\n            raise ValueError(f'Unknown mode {self.mode}')\n</code></pre>"},{"location":"reference/datasets/tasks/#datasets.tasks.Dataset.get_search_service","title":"<code>get_search_service()</code>","text":"<p>Return appropriate search service depending on the search mode</p> Source code in <code>backend/datasets/models.py</code> <pre><code>def get_search_service(self) -&gt; SearchService:\n    \"\"\"\n    Return appropriate search service depending on the search mode\n    \"\"\"\n    match self.search_mode:\n        case self.SearchMode.LOCAL:\n            if not self.search_index_name:\n                raise Exception('Dataset search index has not been created yet')\n            return LocalSearchService(index_name=self.search_index_name)\n        case self.SearchMode.WIKIDATA:\n            return WikidataSearchService()\n        case self.SearchMode.TRIPLYDB:\n            if 'tdb_id' not in self.source:\n                raise Exception('Dataset is not a TriplyDB dataset')\n            return TriplyDBSearchService(self.source['tdb_id'])\n        case _:\n            raise ValueError(f'Unknown search mode {self.search_mode}')\n</code></pre>"},{"location":"reference/datasets/tasks/imports/","title":"imports","text":""},{"location":"reference/datasets/tasks/maintenance/","title":"maintenance","text":""},{"location":"reference/datasets/tasks/pipeline/","title":"pipeline","text":""},{"location":"reference/datasets/tasks/processing/","title":"processing","text":""},{"location":"reference/datasets/views/","title":"views","text":""},{"location":"reference/datasets/views/#datasets.views.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>TaskMixin</code>, <code>TimeStampMixin</code>, <code>OwnableMixin</code></p> <p>The internal dataset model.</p> Source code in <code>backend/datasets/models.py</code> <pre><code>class Dataset(TaskMixin, TimeStampMixin, OwnableMixin):\n    \"\"\"\n    The internal dataset model.\n    \"\"\"\n    STATES = ((state.value, state.value) for state in DatasetState)\n\n    class Mode(models.TextChoices):\n        \"\"\"\n        The Mode class is an enumeration of the possible modes of a dataset\n        \"\"\"\n        LOCAL = 'LOCAL', _('Imported locally ')\n        SPARQL = 'SPARQL', _('From SPARQL endpoint')\n\n    class SearchMode(models.TextChoices):\n        \"\"\"\n        The SearchMode class is an enumeration of the possible search modes of a dataset\n        \"\"\"\n        LOCAL = 'LOCAL', _('Imported locally ')\n        WIKIDATA = 'WIKIDATA', _('From Wikidata')\n        TRIPLYDB = 'TRIPLYDB', _('From TripyDB')\n\n    id = models.UUIDField(default=uuid.uuid4, primary_key=True)\n    \"\"\"The identifier of the dataset.\"\"\"\n    name = models.CharField(max_length=255)\n    \"\"\"The name of the dataset.\"\"\"\n    description = models.TextField(blank=True)\n    \"\"\"The description of the dataset.\"\"\"\n    source = models.JSONField()\n    \"\"\"The source of the dataset.\"\"\"\n    mode = models.CharField(max_length=255, choices=Mode.choices, default=Mode.LOCAL)\n    \"\"\"The mode of the dataset.\"\"\"\n    search_mode = models.CharField(max_length=255, choices=SearchMode.choices, default=SearchMode.LOCAL)\n    \"\"\"The search mode of the dataset.\"\"\"\n    creator = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.SET_NULL, null=True)\n    \"\"\"The user who created the dataset.\"\"\"\n\n    local_database: str = models.CharField(max_length=255, null=True)\n    \"\"\"The local blazegraph database identifier of the dataset.\"\"\"\n    sparql_endpoint = models.CharField(max_length=255, null=True)\n    \"\"\"The SPARQL endpoint of the dataset.\"\"\"\n\n    statistics = models.JSONField(null=True)\n    \"\"\"The statistics of the dataset.\"\"\"\n    namespaces = models.JSONField(null=True)\n    \"\"\"The list of sparql namespaces/prefixes in the dataset.\"\"\"\n    state = models.CharField(choices=STATES, default=DatasetState.QUEUED.value, max_length=255)\n    \"\"\"The import state of the dataset.\"\"\"\n    import_task = models.OneToOneField('tasks.Task', on_delete=models.SET_NULL, null=True)\n    \"\"\"The import task of the dataset.\"\"\"\n\n    objects = models.Manager()\n\n    @property\n    def search_index_name(self) -&gt; str:\n        \"\"\"\n        The path to the search index of the dataset.\n        :return:\n        \"\"\"\n        return self.local_database if self.local_database else None\n\n    def get_search_service(self) -&gt; SearchService:\n        \"\"\"\n        Return appropriate search service depending on the search mode\n        \"\"\"\n        match self.search_mode:\n            case self.SearchMode.LOCAL:\n                if not self.search_index_name:\n                    raise Exception('Dataset search index has not been created yet')\n                return LocalSearchService(index_name=self.search_index_name)\n            case self.SearchMode.WIKIDATA:\n                return WikidataSearchService()\n            case self.SearchMode.TRIPLYDB:\n                if 'tdb_id' not in self.source:\n                    raise Exception('Dataset is not a TriplyDB dataset')\n                return TriplyDBSearchService(self.source['tdb_id'])\n            case _:\n                raise ValueError(f'Unknown search mode {self.search_mode}')\n\n    def get_query_service(self) -&gt; QueryService:\n        \"\"\"\n        If the mode is local, return a local query service, otherwise return a SPARQL query service\n        \"\"\"\n        match self.mode:\n            case self.Mode.LOCAL:\n                if not self.local_database:\n                    raise Exception('Dataset local database has not been imported yet')\n                return LocalQueryService(str(self.local_database))\n            case self.Mode.SPARQL:\n                return SPARQLQueryService(str(self.sparql_endpoint))\n            case _:\n                raise ValueError(f'Unknown mode {self.mode}')\n\n    def can_view(self, user: User):\n        return bool(user)\n\n    def can_edit(self, user: User):\n        return super().can_edit(user) or self.creator == user\n</code></pre>"},{"location":"reference/datasets/views/#datasets.views.Dataset.creator","title":"<code>creator = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.SET_NULL, null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The user who created the dataset.</p>"},{"location":"reference/datasets/views/#datasets.views.Dataset.description","title":"<code>description = models.TextField(blank=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The description of the dataset.</p>"},{"location":"reference/datasets/views/#datasets.views.Dataset.id","title":"<code>id = models.UUIDField(default=uuid.uuid4, primary_key=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The identifier of the dataset.</p>"},{"location":"reference/datasets/views/#datasets.views.Dataset.import_task","title":"<code>import_task = models.OneToOneField('tasks.Task', on_delete=models.SET_NULL, null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The import task of the dataset.</p>"},{"location":"reference/datasets/views/#datasets.views.Dataset.local_database","title":"<code>local_database: str = models.CharField(max_length=255, null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The local blazegraph database identifier of the dataset.</p>"},{"location":"reference/datasets/views/#datasets.views.Dataset.mode","title":"<code>mode = models.CharField(max_length=255, choices=Mode.choices, default=Mode.LOCAL)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The mode of the dataset.</p>"},{"location":"reference/datasets/views/#datasets.views.Dataset.name","title":"<code>name = models.CharField(max_length=255)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the dataset.</p>"},{"location":"reference/datasets/views/#datasets.views.Dataset.namespaces","title":"<code>namespaces = models.JSONField(null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The list of sparql namespaces/prefixes in the dataset.</p>"},{"location":"reference/datasets/views/#datasets.views.Dataset.search_index_name","title":"<code>search_index_name: str</code>  <code>property</code>","text":"<p>The path to the search index of the dataset. :return:</p>"},{"location":"reference/datasets/views/#datasets.views.Dataset.search_mode","title":"<code>search_mode = models.CharField(max_length=255, choices=SearchMode.choices, default=SearchMode.LOCAL)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The search mode of the dataset.</p>"},{"location":"reference/datasets/views/#datasets.views.Dataset.source","title":"<code>source = models.JSONField()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The source of the dataset.</p>"},{"location":"reference/datasets/views/#datasets.views.Dataset.sparql_endpoint","title":"<code>sparql_endpoint = models.CharField(max_length=255, null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The SPARQL endpoint of the dataset.</p>"},{"location":"reference/datasets/views/#datasets.views.Dataset.state","title":"<code>state = models.CharField(choices=STATES, default=DatasetState.QUEUED.value, max_length=255)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The import state of the dataset.</p>"},{"location":"reference/datasets/views/#datasets.views.Dataset.statistics","title":"<code>statistics = models.JSONField(null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The statistics of the dataset.</p>"},{"location":"reference/datasets/views/#datasets.views.Dataset.Mode","title":"<code>Mode</code>","text":"<p>               Bases: <code>TextChoices</code></p> <p>The Mode class is an enumeration of the possible modes of a dataset</p> Source code in <code>backend/datasets/models.py</code> <pre><code>class Mode(models.TextChoices):\n    \"\"\"\n    The Mode class is an enumeration of the possible modes of a dataset\n    \"\"\"\n    LOCAL = 'LOCAL', _('Imported locally ')\n    SPARQL = 'SPARQL', _('From SPARQL endpoint')\n</code></pre>"},{"location":"reference/datasets/views/#datasets.views.Dataset.SearchMode","title":"<code>SearchMode</code>","text":"<p>               Bases: <code>TextChoices</code></p> <p>The SearchMode class is an enumeration of the possible search modes of a dataset</p> Source code in <code>backend/datasets/models.py</code> <pre><code>class SearchMode(models.TextChoices):\n    \"\"\"\n    The SearchMode class is an enumeration of the possible search modes of a dataset\n    \"\"\"\n    LOCAL = 'LOCAL', _('Imported locally ')\n    WIKIDATA = 'WIKIDATA', _('From Wikidata')\n    TRIPLYDB = 'TRIPLYDB', _('From TripyDB')\n</code></pre>"},{"location":"reference/datasets/views/#datasets.views.Dataset.get_query_service","title":"<code>get_query_service()</code>","text":"<p>If the mode is local, return a local query service, otherwise return a SPARQL query service</p> Source code in <code>backend/datasets/models.py</code> <pre><code>def get_query_service(self) -&gt; QueryService:\n    \"\"\"\n    If the mode is local, return a local query service, otherwise return a SPARQL query service\n    \"\"\"\n    match self.mode:\n        case self.Mode.LOCAL:\n            if not self.local_database:\n                raise Exception('Dataset local database has not been imported yet')\n            return LocalQueryService(str(self.local_database))\n        case self.Mode.SPARQL:\n            return SPARQLQueryService(str(self.sparql_endpoint))\n        case _:\n            raise ValueError(f'Unknown mode {self.mode}')\n</code></pre>"},{"location":"reference/datasets/views/#datasets.views.Dataset.get_search_service","title":"<code>get_search_service()</code>","text":"<p>Return appropriate search service depending on the search mode</p> Source code in <code>backend/datasets/models.py</code> <pre><code>def get_search_service(self) -&gt; SearchService:\n    \"\"\"\n    Return appropriate search service depending on the search mode\n    \"\"\"\n    match self.search_mode:\n        case self.SearchMode.LOCAL:\n            if not self.search_index_name:\n                raise Exception('Dataset search index has not been created yet')\n            return LocalSearchService(index_name=self.search_index_name)\n        case self.SearchMode.WIKIDATA:\n            return WikidataSearchService()\n        case self.SearchMode.TRIPLYDB:\n            if 'tdb_id' not in self.source:\n                raise Exception('Dataset is not a TriplyDB dataset')\n            return TriplyDBSearchService(self.source['tdb_id'])\n        case _:\n            raise ValueError(f'Unknown search mode {self.search_mode}')\n</code></pre>"},{"location":"reference/datasets/views/#datasets.views.DatasetViewSet","title":"<code>DatasetViewSet</code>","text":"<p>               Bases: <code>ModelViewSet</code></p> <p>API endpoint that allows users to be viewed or edited.</p> Source code in <code>backend/datasets/views/datasets.py</code> <pre><code>class DatasetViewSet(viewsets.ModelViewSet):\n    \"\"\"\n    API endpoint that allows users to be viewed or edited.\n    \"\"\"\n    queryset = Dataset.objects.all()\n    serializer_class = DatasetSerializer\n    pagination_class = LimitOffsetPagination\n    filter_backends = [DjangoFilterBackend, filters.SearchFilter, filters.OrderingFilter]\n    filterset_fields = ['mode', 'search_mode', 'state', 'id', 'creator']\n    search_fields = ['name', 'source', 'description']\n\n    def perform_create(self, serializer):\n        if serializer.validated_data.get('mode') == Dataset.Mode.SPARQL.value and \\\n                serializer.validated_data.get('search_mode') == Dataset.SearchMode.LOCAL.value:\n            raise ValidationError('Local search index for sparql datasets is not yet supported')\n\n        if serializer.validated_data.get('search_mode', None) == Dataset.SearchMode.TRIPLYDB.value and \\\n                'tdb_id' not in serializer.validated_data.get('source', {}):\n            raise ValidationError('TriplyDB dataset must be a TriplyDB dataset')\n\n        if not settings.BLAZEGRAPH_ENABLE and (\n            serializer.validated_data.get('mode') != Dataset.Mode.SPARQL.value or\n            serializer.validated_data.get('search_mode') == Dataset.SearchMode.LOCAL.value\n        ):\n            raise ValidationError('Local datasets are not enabled on this server')\n\n        super().perform_create(serializer)\n\n        instance: Dataset = serializer.instance\n        instance.creator = self.request.user\n        instance.save()\n\n        files = None\n        # If a files are uploaded, store them in a temporary folder\n        if instance.source.get('source_type') == 'upload':\n            tmp_dir = DOWNLOAD_DIR / random_string(10)\n            tmp_dir.mkdir(parents=True)\n            files = []\n            for file in self.request.FILES.getlist('files'):\n                file_path = tmp_dir / file.name\n                with file_path.open('wb+') as destination:\n                    for chunk in file.chunks():\n                        destination.write(chunk)\n                files.append(str(file_path.absolute()))\n\n        instance.apply_async(\n            import_dataset,\n            (instance.id, files),\n            creator=self.request.user,\n            name=f'Import dataset {instance.name}'\n        )\n\n    def perform_destroy(self, instance):\n        instance.apply_async(\n            delete_dataset,\n            (instance.id,),\n            creator=self.request.user,\n            name=f'Deleting dataset {instance.name}'\n        )\n\n    def perform_update(self, serializer):\n        super().perform_update(serializer)\n\n    def get_permissions(self):\n        permissions = super().get_permissions()\n\n        if self.action in ['destroy']:\n            permissions.append(IsOwner())\n\n        return permissions\n</code></pre>"},{"location":"reference/datasets/views/datasets/","title":"datasets","text":""},{"location":"reference/datasets/views/datasets/#datasets.views.datasets.DatasetViewSet","title":"<code>DatasetViewSet</code>","text":"<p>               Bases: <code>ModelViewSet</code></p> <p>API endpoint that allows users to be viewed or edited.</p> Source code in <code>backend/datasets/views/datasets.py</code> <pre><code>class DatasetViewSet(viewsets.ModelViewSet):\n    \"\"\"\n    API endpoint that allows users to be viewed or edited.\n    \"\"\"\n    queryset = Dataset.objects.all()\n    serializer_class = DatasetSerializer\n    pagination_class = LimitOffsetPagination\n    filter_backends = [DjangoFilterBackend, filters.SearchFilter, filters.OrderingFilter]\n    filterset_fields = ['mode', 'search_mode', 'state', 'id', 'creator']\n    search_fields = ['name', 'source', 'description']\n\n    def perform_create(self, serializer):\n        if serializer.validated_data.get('mode') == Dataset.Mode.SPARQL.value and \\\n                serializer.validated_data.get('search_mode') == Dataset.SearchMode.LOCAL.value:\n            raise ValidationError('Local search index for sparql datasets is not yet supported')\n\n        if serializer.validated_data.get('search_mode', None) == Dataset.SearchMode.TRIPLYDB.value and \\\n                'tdb_id' not in serializer.validated_data.get('source', {}):\n            raise ValidationError('TriplyDB dataset must be a TriplyDB dataset')\n\n        if not settings.BLAZEGRAPH_ENABLE and (\n            serializer.validated_data.get('mode') != Dataset.Mode.SPARQL.value or\n            serializer.validated_data.get('search_mode') == Dataset.SearchMode.LOCAL.value\n        ):\n            raise ValidationError('Local datasets are not enabled on this server')\n\n        super().perform_create(serializer)\n\n        instance: Dataset = serializer.instance\n        instance.creator = self.request.user\n        instance.save()\n\n        files = None\n        # If a files are uploaded, store them in a temporary folder\n        if instance.source.get('source_type') == 'upload':\n            tmp_dir = DOWNLOAD_DIR / random_string(10)\n            tmp_dir.mkdir(parents=True)\n            files = []\n            for file in self.request.FILES.getlist('files'):\n                file_path = tmp_dir / file.name\n                with file_path.open('wb+') as destination:\n                    for chunk in file.chunks():\n                        destination.write(chunk)\n                files.append(str(file_path.absolute()))\n\n        instance.apply_async(\n            import_dataset,\n            (instance.id, files),\n            creator=self.request.user,\n            name=f'Import dataset {instance.name}'\n        )\n\n    def perform_destroy(self, instance):\n        instance.apply_async(\n            delete_dataset,\n            (instance.id,),\n            creator=self.request.user,\n            name=f'Deleting dataset {instance.name}'\n        )\n\n    def perform_update(self, serializer):\n        super().perform_update(serializer)\n\n    def get_permissions(self):\n        permissions = super().get_permissions()\n\n        if self.action in ['destroy']:\n            permissions.append(IsOwner())\n\n        return permissions\n</code></pre>"},{"location":"reference/datasets/views/lodc/","title":"lodc","text":""},{"location":"reference/reports/","title":"reports","text":""},{"location":"reference/reports/consumers/","title":"consumers","text":""},{"location":"reference/reports/models/","title":"models","text":""},{"location":"reference/reports/permissions/","title":"permissions","text":""},{"location":"reference/reports/permissions/#reports.permissions.CanEditReport","title":"<code>CanEditReport</code>","text":"<p>               Bases: <code>BasePermission</code></p> <p>Custom permission to only allow owners of an object to edit it.</p> Source code in <code>backend/reports/permissions.py</code> <pre><code>class CanEditReport(permissions.BasePermission):\n    \"\"\"\n    Custom permission to only allow owners of an object to edit it.\n    \"\"\"\n\n    def has_permission(self, request, view):\n        return request.user and request.user.is_authenticated\n\n    def has_object_permission(self, request, view, obj: Report):\n        return obj.can_edit(request.user)\n</code></pre>"},{"location":"reference/reports/permissions/#reports.permissions.CanViewReport","title":"<code>CanViewReport</code>","text":"<p>               Bases: <code>BasePermission</code></p> <p>Custom permission to only allow owners of an object to edit it.</p> Source code in <code>backend/reports/permissions.py</code> <pre><code>class CanViewReport(permissions.BasePermission):\n    \"\"\"\n    Custom permission to only allow owners of an object to edit it.\n    \"\"\"\n\n    def has_permission(self, request, view):\n        return request.user and request.user.is_authenticated\n\n    def has_object_permission(self, request, view, obj: Report):\n        return obj.can_view(request.user)\n</code></pre>"},{"location":"reference/reports/serializers/","title":"serializers","text":""},{"location":"reference/reports/tasks/","title":"tasks","text":""},{"location":"reference/reports/views/","title":"views","text":""},{"location":"reference/reports/views/#reports.views.ReportViewSet","title":"<code>ReportViewSet</code>","text":"<p>               Bases: <code>ModelViewSet</code></p> <p>API endpoint that allows users to be viewed or edited.</p> Source code in <code>backend/reports/views.py</code> <pre><code>class ReportViewSet(viewsets.ModelViewSet):\n    \"\"\"\n    API endpoint that allows users to be viewed or edited.\n    \"\"\"\n    queryset = Report.objects.all()\n    serializer_class = ReportSerializer\n    pagination_class = LimitOffsetPagination\n    filter_backends = [DjangoFilterBackend, filters.SearchFilter, filters.OrderingFilter]\n    search_fields = ['notebook']\n\n    def perform_create(self, serializer):\n        serializer.save(dataset_id=self.request.data['dataset'])\n\n        instance: Report = serializer.instance\n        instance.creator = self.request.user\n        instance.save()\n\n    def get_queryset(self):\n        if self.request.user.is_superuser:\n            return super().get_queryset()\n\n        return super().get_queryset().filter(\n            Q(creator=self.request.user) | (Q(discoverable=True) &amp; ~Q(share_mode=Report.ShareModes.PRIVATE))\n        )\n\n    def get_permissions(self):\n        permissions = super().get_permissions()\n\n        if self.action in ['update', 'partial_update']:\n            permissions.append(CanEditReport())\n\n        if self.action in ['destroy']:\n            permissions.append(IsOwner())\n\n        if self.action in ['retrieve']:\n            permissions.append(CanViewReport())\n\n        return permissions\n</code></pre>"},{"location":"reference/shared/","title":"shared","text":""},{"location":"reference/shared/dict/","title":"dict","text":""},{"location":"reference/shared/dict/#shared.dict.deepget","title":"<code>deepget(data, key, default=None)</code>","text":"<p>It takes a dictionary, a list of keys, and a default value, and returns the value of the key in the dictionary, or the default value if the key is not found</p> <p>:param data: The data to search through :param key: The key to search for :param default: The default value to return if the key is not found :return: The value of the key in the data.</p> Source code in <code>backend/shared/dict.py</code> <pre><code>def deepget(data, key, default=None):\n    \"\"\"\n    It takes a dictionary, a list of keys, and a default value, and returns the value of the key in the dictionary, or the\n    default value if the key is not found\n\n    :param data: The data to search through\n    :param key: The key to search for\n    :param default: The default value to return if the key is not found\n    :return: The value of the key in the data.\n    \"\"\"\n    try:\n        return reduce(getitem, key, data)\n    except (KeyError, IndexError):\n        return default\n</code></pre>"},{"location":"reference/shared/logging/","title":"logging","text":""},{"location":"reference/shared/logging/#shared.logging.get_logger","title":"<code>get_logger(name=None)</code>","text":"<p>It returns a global logger object</p> <p>:param name: The name of the logger. If you don't specify a name, the root logger will be returned :return: A logger object.</p> Source code in <code>backend/shared/logging.py</code> <pre><code>def get_logger(name=None):\n    \"\"\"\n    It returns a global logger object\n\n    :param name: The name of the logger. If you don't specify a name, the root logger will be returned\n    :return: A logger object.\n    \"\"\"\n    return logging.getLogger(name or 'root')\n</code></pre>"},{"location":"reference/shared/models/","title":"models","text":""},{"location":"reference/shared/models/#shared.models.TimeStampMixin","title":"<code>TimeStampMixin</code>","text":"<p>               Bases: <code>Model</code></p> <p>This class is a mixin that adds created_at and updated_at fields to any model that inherits from it.</p> Source code in <code>backend/shared/models.py</code> <pre><code>class TimeStampMixin(models.Model):\n    \"\"\"\n    This class is a mixin that adds created_at and updated_at fields to any model that inherits from it.\n    \"\"\"\n    created_at = models.DateTimeField(auto_now_add=True, editable=False)\n    \"\"\"The date and time when the object was created.\"\"\"\n    updated_at = models.DateTimeField(auto_now=True)\n    \"\"\"The date and time when the object was last updated.\"\"\"\n\n    class Meta:\n        abstract = True\n</code></pre>"},{"location":"reference/shared/models/#shared.models.TimeStampMixin.created_at","title":"<code>created_at = models.DateTimeField(auto_now_add=True, editable=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The date and time when the object was created.</p>"},{"location":"reference/shared/models/#shared.models.TimeStampMixin.updated_at","title":"<code>updated_at = models.DateTimeField(auto_now=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The date and time when the object was last updated.</p>"},{"location":"reference/shared/paths/","title":"paths","text":""},{"location":"reference/shared/paths/#shared.paths.BIN_DIR","title":"<code>BIN_DIR = settings.BASE_DIR / 'backend' / 'bin'</code>  <code>module-attribute</code>","text":"<p>The path to the backend/bin directory.</p>"},{"location":"reference/shared/paths/#shared.paths.DATA_DIR","title":"<code>DATA_DIR = settings.STORAGE_DIR / 'data'</code>  <code>module-attribute</code>","text":"<p>The path to the data directory.</p>"},{"location":"reference/shared/paths/#shared.paths.DEFAULT_SEARCH_INDEX_NAME","title":"<code>DEFAULT_SEARCH_INDEX_NAME = 'search_index_default'</code>  <code>module-attribute</code>","text":"<p>The name of the default search index.</p>"},{"location":"reference/shared/paths/#shared.paths.DOWNLOAD_DIR","title":"<code>DOWNLOAD_DIR = settings.STORAGE_DIR / 'downloads'</code>  <code>module-attribute</code>","text":"<p>The path to the downloads directory.</p>"},{"location":"reference/shared/paths/#shared.paths.EXPORT_DIR","title":"<code>EXPORT_DIR = settings.STORAGE_DIR / 'export'</code>  <code>module-attribute</code>","text":"<p>The path to the export directory.</p>"},{"location":"reference/shared/paths/#shared.paths.IMPORT_DIR","title":"<code>IMPORT_DIR = settings.STORAGE_DIR / 'import'</code>  <code>module-attribute</code>","text":"<p>The path to the import directory.</p>"},{"location":"reference/shared/query/","title":"query","text":""},{"location":"reference/shared/query/#shared.query.q_json_update","title":"<code>q_json_update(field, key, value)</code>","text":"<p>It takes a JSON field, a list of keys, and a value, and returns a RawSQL object that will update the JSON field with the value at the given keys</p> <p>:param field: The field to update :type field: str :param key: The key to update :type key: List[str] :param value: The value to be inserted into the JSON field :type value: Any :return: A RawSQL object.</p> Source code in <code>backend/shared/query.py</code> <pre><code>def q_json_update(field: str, key: List[str], value: Any) -&gt; RawSQL:\n    \"\"\"\n    It takes a JSON field, a list of keys, and a value, and returns a RawSQL object that will update the JSON field with the\n    value at the given keys\n\n    :param field: The field to update\n    :type field: str\n    :param key: The key to update\n    :type key: List[str]\n    :param value: The value to be inserted into the JSON field\n    :type value: Any\n    :return: A RawSQL object.\n    \"\"\"\n    key = ','.join(key)\n\n    return RawSQL(f'''\n            jsonb_set({field}::jsonb, %s::text[], %s::jsonb, true)\n        ''', [\n        f'{{{key}}}',\n        json.dumps(value),\n    ])\n</code></pre>"},{"location":"reference/shared/random/","title":"random","text":""},{"location":"reference/shared/random/#shared.random.random_string","title":"<code>random_string(length=10)</code>","text":"<p>Generates a random string of letters and digits of the specified length.</p> Source code in <code>backend/shared/random.py</code> <pre><code>def random_string(length=10) -&gt; str:\n    \"\"\"\n    Generates a random string of letters and digits of the specified length.\n    \"\"\"\n    return ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))\n</code></pre>"},{"location":"reference/shared/shell/","title":"shell","text":""},{"location":"reference/shared/shell/#shared.shell.consume_print","title":"<code>consume_print(it)</code>","text":"<p>It takes an iterator of strings and prints each line to the log</p> <p>:param it: Iterator[str] :type it: Iterator[str]</p> Source code in <code>backend/shared/shell.py</code> <pre><code>def consume_print(it: Iterator[str]):\n    \"\"\"\n    It takes an iterator of strings and prints each line to the log\n\n    :param it: Iterator[str]\n    :type it: Iterator[str]\n    \"\"\"\n    for line in it:\n        logger.info(line)\n</code></pre>"},{"location":"reference/shared/shell/#shared.shell.execute_command","title":"<code>execute_command(cmd, timeout=None, ignore_errors=False, **kwargs)</code>","text":"<p>It runs a command and yields the output line by line</p> <p>:param cmd: The command to execute :type cmd: Union[List[str], str] :param timeout: The maximum time to wait for the command to finish :type timeout: int :param ignore_errors: If True, don't raise an exception if the command fails, defaults to False :type ignore_errors: bool (optional)</p> Source code in <code>backend/shared/shell.py</code> <pre><code>def execute_command(\n    cmd: Union[List[str], str],\n    timeout: int = None,\n    ignore_errors: bool = False,\n    **kwargs,\n) -&gt; Iterator[str]:\n    \"\"\"\n    It runs a command and yields the output line by line\n\n    :param cmd: The command to execute\n    :type cmd: Union[List[str], str]\n    :param timeout: The maximum time to wait for the command to finish\n    :type timeout: int\n    :param ignore_errors: If True, don't raise an exception if the command fails, defaults to False\n    :type ignore_errors: bool (optional)\n    \"\"\"\n    p = subprocess.Popen(\n        cmd,\n        shell=False,\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n        **kwargs\n    )\n\n    def run():\n        for line in iter(p.stdout.readline, b''):\n            line = line.rstrip().decode('utf-8')\n            yield line\n\n        p.stdout.close()\n        code = p.wait()\n        if code != 0 and not ignore_errors:\n            raise CommandFailed(code)\n\n    if timeout is not None:\n        with with_timeout(p, timeout):\n            yield from run()\n    else:\n        yield from run()\n</code></pre>"},{"location":"reference/shared/shell/#shared.shell.with_timeout","title":"<code>with_timeout(p, timeout)</code>","text":"<p>It runs a function in a separate thread, and if the function doesn't return before the timeout, it kills the process</p> <p>:param p: subprocess.Popen :type p: subprocess.Popen :param timeout: The maximum time to wait for the process to finish :type timeout: int</p> Source code in <code>backend/shared/shell.py</code> <pre><code>@contextmanager\ndef with_timeout(p: subprocess.Popen, timeout: int):\n    \"\"\"\n    It runs a function in a separate thread, and if the function doesn't return before the timeout, it kills the process\n\n    :param p: subprocess.Popen\n    :type p: subprocess.Popen\n    :param timeout: The maximum time to wait for the process to finish\n    :type timeout: int\n    \"\"\"\n    def timerout(p: subprocess.Popen):\n        p.kill()\n        raise TimeoutError\n\n    timer = Timer(timeout, timerout, [p])\n    timer.start()\n    yield\n    timer.cancel()\n</code></pre>"},{"location":"reference/shared/websocket/","title":"websocket","text":""},{"location":"reference/shared/websocket/#shared.websocket.Packet","title":"<code>Packet</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Serializable</code></p> <p>Packet class represents a message that is sent through the websocket.</p> Source code in <code>backend/shared/websocket.py</code> <pre><code>@dataclass\nclass Packet(Serializable):\n    \"\"\"\n    Packet class represents a message that is sent through the websocket.\n    \"\"\"\n    type: str\n    \"\"\"The type of the packet.\"\"\"\n    data: Generic[T] = None\n    \"\"\"The data field is the actual data that is sent through the websocket.\"\"\"\n\n    def to_dict(self, dict_factory: Type[Dict] = dict, recurse: bool = True) -&gt; Dict:\n        return {\n            'type': str(self.type),\n            'data': self.data.to_dict() if isinstance(self.data, Serializable) else self.data,\n        }\n</code></pre>"},{"location":"reference/shared/websocket/#shared.websocket.Packet.data","title":"<code>data: Generic[T] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The data field is the actual data that is sent through the websocket.</p>"},{"location":"reference/shared/websocket/#shared.websocket.Packet.type","title":"<code>type: str</code>  <code>instance-attribute</code>","text":"<p>The type of the packet.</p>"},{"location":"reference/tasks/","title":"tasks","text":""},{"location":"reference/tasks/apps/","title":"apps","text":""},{"location":"reference/tasks/consumers/","title":"consumers","text":""},{"location":"reference/tasks/models/","title":"models","text":""},{"location":"reference/tasks/models/#tasks.models.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>TimeStampMixin</code></p> Source code in <code>backend/tasks/models.py</code> <pre><code>class Task(TimeStampMixin):\n    STATES = (\n        (TaskState.PENDING, 'PENDING'),\n        (TaskState.STARTED, 'STARTED'),\n        (TaskState.RETRY, 'RETRY'),\n        (TaskState.FAILURE, 'FAILURE'),\n        (TaskState.SUCCESS, 'SUCCESS'),\n    )\n\n    task_id = models.UUIDField(primary_key=True)\n    state = models.CharField(choices=STATES, default=TaskState.PENDING, max_length=255)\n    object_id = models.UUIDField(null=True)\n    content_object = GenericForeignKey()\n    content_type = models.ForeignKey(ContentType, null=True, on_delete=models.SET_NULL)\n    name = models.CharField(max_length=255)\n    creator = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.SET_NULL, null=True)\n    \"\"\"The user who started the task.\"\"\"\n\n    objects = TaskManager()\n\n    def __str__(self):\n        return '%s: %s' % (self.task_id, dict(self.STATES)[self.state])\n\n    @property\n    def result(self):\n        return ModelAsyncResult(self.task_id)\n\n    @property\n    def short_id(self):\n        return str(self.task_id)[:6]\n\n    def delete(self, using=None, keep_parents=False):\n        return super().delete(using, keep_parents)\n\n    def can_view(self, user: User):\n        return user.is_superuser or self.creator == user or (\n            self.content_object and self.content_object.can_view(user)\n        )\n</code></pre>"},{"location":"reference/tasks/models/#tasks.models.Task.creator","title":"<code>creator = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.SET_NULL, null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The user who started the task.</p>"},{"location":"reference/tasks/serializers/","title":"serializers","text":""},{"location":"reference/tasks/signals/","title":"signals","text":""},{"location":"reference/tasks/utils/","title":"utils","text":""},{"location":"reference/tasks/views/","title":"views","text":""},{"location":"reference/tasks/views/#tasks.views.TaskViewSet","title":"<code>TaskViewSet</code>","text":"<p>               Bases: <code>ModelViewSet</code></p> <p>API endpoint that allows users to be viewed or edited.</p> Source code in <code>backend/tasks/views.py</code> <pre><code>class TaskViewSet(viewsets.ModelViewSet):\n    \"\"\"\n    API endpoint that allows users to be viewed or edited.\n    \"\"\"\n    queryset = Task.objects.all()\n    serializer_class = TaskSerializer\n    pagination_class = LimitOffsetPagination\n    filter_backends = [DjangoFilterBackend, filters.SearchFilter, filters.OrderingFilter]\n    search_fields = ['object_id', 'name', 'state']\n    filter_class = TaskFilter\n\n    def get_queryset(self):\n        if self.request.user.is_superuser:\n            return super().get_queryset()\n\n        return super().get_queryset().filter(\n            Q(creator=self.request.user)\n        )\n</code></pre>"},{"location":"reference/tasks/management/","title":"management","text":""},{"location":"reference/tasks/management/commands/","title":"commands","text":""},{"location":"reference/tasks/management/commands/celery_worker/","title":"celery_worker","text":""},{"location":"reference/users/","title":"users","text":""},{"location":"reference/users/middleware/","title":"middleware","text":""},{"location":"reference/users/mixins/","title":"mixins","text":""},{"location":"reference/users/permissions/","title":"permissions","text":""},{"location":"reference/users/permissions/#users.permissions.IsOwner","title":"<code>IsOwner</code>","text":"<p>               Bases: <code>BasePermission</code></p> <p>Custom permission to only allow owners of an object to edit it.</p> Source code in <code>backend/users/permissions.py</code> <pre><code>class IsOwner(permissions.BasePermission):\n    \"\"\"\n    Custom permission to only allow owners of an object to edit it.\n    \"\"\"\n\n    def has_permission(self, request, view):\n        return request.user and request.user.is_authenticated\n\n    def has_object_permission(self, request, view, obj):\n        return obj.creator == request.user or request.user.is_superuser\n</code></pre>"},{"location":"reference/users/serializers/","title":"serializers","text":""},{"location":"reference/users/views/","title":"views","text":""}]}